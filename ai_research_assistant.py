# -*- coding: utf-8 -*-
"""Ai research assistant

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gB91WMDB8lmGJWyP8oZmA8cgmCgU6b2v
"""

!pip install -U google-generativeai langgraph

import google.generativeai as genai

GEMINI_API_KEY = "AIzaSyBGu_Ealjb1br4RRVKv3cmVZOkKU4ScgBo"

genai.configure(api_key=GEMINI_API_KEY)

def call_llm(prompt: str) -> str:
    model = genai.GenerativeModel("gemini-2.5-flash")

    response = model.generate_content(
        prompt,
        generation_config={"temperature": 0.4}
    )

    return response.text

def step_research(topic):
    prompt = f"Research this topic deeply and gather detailed, factual information:\n\nTopic: {topic}"
    return {"research": call_llm(prompt)}

def step_summarize(research_text):
    prompt = f"Summarize the following research in a clear and concise way:\n\n{research_text}"
    return {"summary": call_llm(prompt)}

def step_review(summary):
    prompt = f"Review this summary for clarity, correctness, and completeness:\n\n{summary}"
    return {"review": call_llm(prompt)}

from dataclasses import dataclass
from langgraph.graph import StateGraph, END

@dataclass
class ResearchState:
    topic: str = ""
    research: str = ""
    summary: str = ""
    review: str = ""

def build_research_graph():
    graph = StateGraph(ResearchState)

    graph.add_node("research", lambda s: step_research(s.topic))
    graph.add_node("summarize", lambda s: step_summarize(s.research))
    graph.add_node("review", lambda s: step_review(s.summary))

    graph.set_entry_point("research")

    graph.add_edge("research", "summarize")
    graph.add_edge("summarize", "review")
    graph.add_edge("review", END)

    return graph.compile()

def run_research_agent(topic: str):
    state = ResearchState(topic=topic)
    result = research_agent.invoke(state)

    # result is a dict â€” access keys directly
    return result.get("review", "No review field returned.")

output = run_research_agent("How does LangGraph work?")
print(output)

# List of sample research questions
questions = [
    "Explain the main differences between LangGraph and LangChain.",
    "How do autonomous AI agents work in industry applications?",
    "Summarize recent advancements in large language model memory techniques.",
    "Provide an overview of vector databases used for AI retrieval-augmented generation.",
    "Explain the ethical considerations of using AI agents for business decision-making.",
    "What are the key techniques in prompt engineering for AI agents?",
    "How can AI agents improve software development workflows?",
    "Explain multi-agent reinforcement learning in simple terms.",
    "What are the challenges in deploying LLM-based AI agents in production?",
    "Describe how RAG (retrieval-augmented generation) works in AI systems."
]

# Loop over the questions and print results
for q in questions:
    print(f"\n=== Question: {q} ===\n")
    output = run_research_agent(q)
    print(output[:1000])  # print first 1000 characters
    print("\n" + "-"*80)

def save_agent(filepath="saved_model/research_agent.pkl"):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "wb") as f:
        pickle.dump(research_agent, f)

def load_agent(filepath="saved_model/research_agent.pkl"):
    with open(filepath, "rb") as f:
        return pickle.load(f)